# 机器学习

## 机器学习三要素

1. 模型
2. 学习准则: 期望风险最小
3. 优化: 梯度下降

## 损失函数

### 0-1 损失函数

$$
\mathcal{L}(y,f(x,\theta))=
\begin{cases}
    0,& \text{if }y=f(x,\theta) \\
    1,& \text{if }y\neq f(x,\theta)
\end{cases}
$$

### 平方损失函数

$$
\mathcal{L}(y,\hat{y})=(y-f(x,\theta))^2
$$

## 经验风险

$$
R^{emp}_ D(\theta)=\frac{1}{N}\sum_ {n=1}^{N}L(y^{(n)},f(x^{(n)},\theta))
$$

## 随机梯度下降法

$$
\theta_{t+1}=\theta_t-\alpha \frac{\partial \mathcal{L}(\theta_t;x^{(t)}, y^{(t)})}{\partial\theta}
$$

> [!NOTE]
> 衍生算法: 小批量随机梯度下降法

## 泛化错误

$$
\text{泛化错误}=\text{期望风险}-\text{经验风险}
$$

$$
\mathcal{G}_ {\mathcal{D}}(f)=\mathcal{R}(f)-\mathcal{R}^{emp}_ {\mathcal{D}}(f)
$$

## 正则化

所有损害优化的方法都是正则化

- 增加优化约束
  - L1/L2 约束数据增强
- 干扰优化过程
  - 权重衰减
  - 提前停止
  - 随机梯度下降

## 优化方法

- 经验风险最小化
  - 最小二乘法
- 结构风险最小化
  - 岭回归
- 最大似然估计
- 最大后验估计

# 前馈神经网络

神经网络最早是作为一种主要的连接主义模型,引入误差反向传播来改进其学习能力之后，神经网络也越来越多地应用在各种机器学习任务上,神经网络有三个主要特性：

- 信息表示是分布式的（非局部的）
- 记忆和知识是存储在单元之间的连接上
- 通过逐渐改变单元之间的连接强度来学习新的知识

## 激活函数性质

- 连续并可导（允许少数点上不可导）的非线性函数
- 激活函数及其导函数要尽可能的简单
- 激活函数的导函数的值域要在一个合适的区间内
- 单调递增

## 常见激活函数及对应导数

| 激活函数        | 函数                                               | 导数                                                 |
|-----------------|----------------------------------------------------|------------------------------------------------------|
| $Logistic$ 函数 | $f(x) = \frac{1}{1 + exp(-x)}$                     | $f'(x) = f(x)(1 - f(x))$                             |
| $Tanh$ 函数     | $f(x) = \frac{exp(x) - exp(-x)}{exp(x) + exp(-x)}$ | $f'(x) = 1 - f(x)^2$                                 |
| $ReLU$ 函数     | $f(x) = max(0, x)$                                 | $f'(x) = I(x > 0)$                                   |
| $ELU$ 函数      | $f(x) = max(0, x) + min(0, \gamma(exp(x) – 1))$    | $f'(x) = I(x > 0) + I(x \leq 0) \cdot \gamma exp(x)$ |
| $SoftPlus$ 函数 | $f(x) = log (1 + exp(x))$                          | $f'(x) = \frac{1}{1 + exp(-x)}$                      |

[详情信息](https://www.jiqizhixin.com/articles/2021-02-24-7)

## 前馈网络细节（全连接神经网络、多层感知器）

- 各神经元分别属于不同的层，层内无连接
- 相邻两层之间的神经元全部两两连接
- 整个网络中无反馈，信号从输入层向输出层单向传播，可用一个有向无环图表示

## 前馈计算

![前馈网络](https://i-blog.csdnimg.cn/blog_migrate/5264fc3b3bd415b25d73c40146f549a1.png)

| 记号                                                   | 含义                               |
|--------------------------------------------------------|------------------------------------|
| $L$                                                    | 神经网络的层数                     |
| $M_l$                                                  | 第 $l$ 层神经元的个数              |
| $f_l(\cdot)$                                           | 第 $l$ 层神经元的激活函数          |
| $\mathbf{W}^{(l)} \in \mathbb{R}^{M_l \times M_{l-1}}$ | 第 $l-1$ 层到第 $l$ 层的权重矩阵   |
| $\mathbf{b}^{(l)} \in \mathbb{R}^{M_l}$                | 第 $l-1$ 层到第 $l$ 层的偏置       |
| $\mathbf{z}^{(l)} \in \mathbb{R}^{M_l}$                | 第 $l$ 层神经元的净输入 (净活化值) |
| $\mathbf{a}^{(l)} \in \mathbb{R}^{M_l}$                | 第 $l$ 层神经元的输出 (活化值)     |

前馈传播公式：

$$
\begin{cases}
& z^{(l)}=W^{(l)}\cdot a^{(l-1)} + b^{(l)} \\
& a^{(l)}=f_ {l}(z^{(l)}) \\
\end{cases}
$$

计算过程：

$$
x=a^{(0)} \rightarrow z^{(1)} \rightarrow a^{(1)} \rightarrow z^{(2)} \rightarrow a^{(2)} \rightarrow \cdots \rightarrow z^{(L)} \rightarrow a^{(L)} = \phi(x;(W,b))
$$

## 通用近似定理

根据通用近似定理，对于具有线性输出层和至少一个使用“挤压”性质的激活函数的隐藏层组成的前馈神经网络，只要其隐藏层神经元的
数量足够，它可以以任意的精度来近似任何从一个定义在实数空间中的有界闭集函数

## 对于机器学习的应用

- 特征转换，或是逼近某一种分布
- 最后一层加 $Logistic$ 可作为分类器
- 最后一层加 $Softmax$ 可输出每个类的条件概率
- 对于样本 $(x,y)$ , 常使用交叉熵损失函数 $\mathcal{L}(y,\hat{y}) = -y^{T}logy$

## 参数学习

$$
R(W,b) = \frac{1}{N}\sum_ {n=1}^{N}\mathcal{L}(y^{(n)},\hat{y}^{(n)})+\frac{1}{2}\lambda\parallel W \parallel^2_ F
$$

$$
W^{(l)} \leftarrow W^{(l)} - \alpha \frac{\partial R(W,b)}{\partial W^{(l)}}
$$

$$
b^{(l)} \leftarrow b^{(l)} - \alpha \frac{\partial R(W,b)}{\partial b^{(l)}}
$$

# 反向传播算法

## 链式法则

直接用损失函数对权值求导太复杂，因此先对净活化值 $z$ 求导，分布进行

$$
\begin{aligned}
\frac{\partial \mathbf{z}^{(l)}}{\partial w_ {ij}^{(l)}} &= \left[ \frac{\partial z_ 1^{(l)}}{\partial w_ {ij}^{(l)}}, \dots, \frac{\partial z_ i^{(l)}}{\partial w_ {ij}^{(l)}}, \dots, \frac{\partial z_ {m^{(l)}}^{(l)}}{\partial w_ {ij}^{(l)}} \right] \\
&= \left[ 0, \dots, \frac{\partial (\mathbf{w}_ {i:}^{(l)} \mathbf{a}^{(l-1)} + b_ i^{(l)})}{\partial w_ {ij}^{(l)}}, \dots, 0 \right] \\
&= \left[ 0, \dots, a_ j^{(l-1)}, \dots, 0 \right] \\
&\triangleq \mathbb{I}_ i (a_ j^{(l-1)}) \in \mathbb{R}^{m^{(l)}}, \\\\
\frac{\partial \mathbf{z}^{(l)}}{\partial \mathbf{b}^{(l)}} &= \mathbf{I}_ {m^{(l)}} \in \mathbb{R}^{m^{(l)} \times m^{(l)}}
\end{aligned}
$$

然后使用链式法则

$$
\begin{aligned}
\frac{\partial \mathcal{L}(y, \hat{y})}{\partial w_{ij}^{(l)}} = \frac{\partial z^{(l)}}{\partial w_{ij}^{(l)}} \frac{\partial \mathcal{L}(y, \hat{y})}{\partial z^{(l)}} \\
\frac{\partial \mathcal{L}(y, \hat{y})}{\partial b^{(l)}} = \frac{\partial z^{(l)}}{\partial b^{(l)}} \frac{\partial \mathcal{L}(y, \hat{y})}{\partial z^{(l)}}
\end{aligned}
$$

$$
误差项: \delta^{(l)}=\frac{\partial\mathcal{L}(y,\hat{{y}})}{\partial z^{(l)}} \in \mathbb{R}^{m^{(l)}}
$$

通过链式法则得到损失函数对 $l$ 层权重 $W^{(l)}$ 的梯度：

$$
\frac{\partial\mathcal{L}(y,\hat{y})}{\partial w_ {ij}^{(l)}} = \delta^{(l)}(a^{(l-1)})^T
$$

同理， $\mathcal{L}(y,\hat{y})$ 关于第 $l$ 层偏置 $b^{(l)}$ 的梯度为：

$$
\frac{\partial\mathcal{L}(y,\hat{y})}{\partial b^{(l)}} = \delta^{(l)}
$$

## 算法流程

1. 输入: 训练集 $D = \{(x^{(n)}, y^{(n)})\}_{n=1}^{N}$, 验证集 $V$, 学习率 $\alpha$, 正则化系数 $\lambda$, 网络层数 $L$, 神经元数量 $M_l(1 \leq l \leq L)$
2. 随机初始化 $W, b$
3. repeat
    1. 对训练集 $D$ 中的样本随机重排序
    2. for $n = 1$ to $N$ do
        - 从训练集 $D$ 中选取样本 $(x^{(n)}, y^{(n)})$
        - 前馈计算每一层的净输入 $z^{(l)}$ 和激活值 $a^{(l)}$, 直到最后一层
        - 反向传播计算每一层的误差 $\delta^{(l)}$
        - 计算每一层参数的导数
            - $\forall l, \frac{\partial \mathcal{L}(y^{(n)}, \hat{y}^{(n)})}{\partial W^{(l)}} = \delta^{(l)} (a^{(l-1)})^T$
            - $\forall l, \frac{\partial \mathcal{L}(y^{(n)}, \hat{y}^{(n)})}{\partial b^{(l)}} = \delta^{(l)}$
        - 更新参数
            - $W^{(l)} \leftarrow W^{(l)} - \alpha (\delta^{(l)} (a^{(l-1)})^T + \lambda W^{(l)})$
            - $b^{(l)} \leftarrow b^{(l)} - \alpha \delta^{(l)}$
    3. end for
4. **until** 神经网络模型在验证集 $V$ 上的错误率不再下降
5. 输出: $W, b$

## 自动微分

通过将数值计算分解为一系列基本运算，然后利用链式法则自动地计算这些基本运算的导数，从而得到整个函数的导数

## 优化问题

- 参数过多, 影响训练
- 非凸优化问题
- 梯度消失问题
- 参数解释起来比较困难

# 卷积神经网络

### 特性

- 局部连接
- 权重共享
- 空间或时间上的次采样

### 卷积结果按输出长度不同可以分为三类

设步长 $S$, 输入长度 $n$, 卷积核长度 $m$

- 窄卷积: $s=1$, 两端不补零 $p=0$, 卷积结果长度 $n-m+1$
- 宽卷积: $s=1$, 两端不补零 $p=m-1$, 卷积结果长度 $n+m-1$
- 等宽卷积: $s=1$, 两端不补零 $p=\frac{m-1}{2}$, 卷积结果长度 $n$

> [!NOTE]
> **卷积**运算需要翻转卷积核，**互相关**运算不需要翻转核；而卷积网络中卷积核为可学习参数，因此两者等价。卷积网络中的卷积一般指互相关

## 卷积层

卷积层的作用是提取一个局部区域的特征，不同的卷积核相当于不同的特征提取器，在图像卷积中，为了更充分地利用图像地局部信息，通常将神经元组织为三维结构的神经层，其大小为高度 $M \times$ 宽度 $N \times$ 深度 $D$, 有 $D$ 个 $M \times N$ 大小地特征映射构成

在输入层，特征映射就是图像本身，如果是灰度图像，就是有一个特征映射，深度 $D=1$; 如果是彩色图像，分别有 RGB 三个颜色通道的特征映射，输入层深度 $D=3$

假设一个卷积层结构如下：

- 输入特征映射组: $X\in R^{M\times N\times D}$ 为三维张量, 其中每个切片矩阵 $X^d\in R^{M\times N}$ 为一个输入特征映射, $1\leqslant d\leqslant D$
- 输出特征映射组: $Y\in R^{M'\times N'\times p}$ 为三维张量, 其中每个切片矩阵 $Y^d\in R^{M'\times N'}$ 为一个输出特征映射, $1\leqslant p \leqslant P$
- 卷积核: $W\in R^{m\times n\times D\times P}$ 为四维张量, 其中每个切片矩阵 $W^{p,d}\in R^{m\times n}$ 为一个两维卷积核, $1\leqslant D, 1\leqslant p \leqslant P$

最终表达式

$$
\begin{align}
  & Z^p=W^p \otimes X+b^p=\sum_{d=1}^{D} W^{p,d}\otimes X^d +b^p \\
  & Y^p=f(Z^p) \\
\end{align}
$$

$f(\cdot)$ 为激活函数

## 池化层 (汇聚层)

作用: 降低特征维数，避免过拟合

常用的汇聚函数
- 最大汇聚 (Maximun Pooling): 取一个区域内所有神经元的最大值
- 平均汇聚 (Mean Pooling): 一般是取区域内所有神经元的平均值

## 反向传播算法

### 卷积层

考虑第 $l$ 层卷积层，有

$$
Z^{(l,p)}=\sum^D_ {d=1}W^{(l,p,d)}\otimes X^{l-1,d} +b^{(l,d)}
$$

则偏导数

$$
\begin{align}
 & \frac{\partial\mathcal{L}(Y,\hat{Y})}{\partial W^{(l,p,d)}}=\frac{\partial\mathcal{L}(Y,\hat{Y})}{\partial Z^{(l,p)}} \otimes X^{(l-1,d)}=\delta^{(l,p)}\otimes X^{(l-1,d)} \\
 & \frac{\partial\mathcal{L}(Y,\hat{Y})}{\partial b^{(l,p)}}=\sum_ x \sum_ y \delta^{(l,p)}
\end{align}
$$

其中，误差项 $\delta^{(l,p)}$ 满足递推关系

$$
\begin{aligned}
\delta^{(l,p)} &=\frac{\partial\mathcal{L}(Y,\hat{Y})}{\partial Z^{(l,p)}} \\
&=\frac{\partial X^{(l,d)}}{\partial Z^{(l,d)}} \frac{\partial Z^{l+1,p}}{\partial X^{l,p}} \frac{\partial \mathcal{L}(Y,\hat{Y})}{\partial Z^{(l+1,p)}} \\
&=f^{\prime}_ l \cdot ROT180(W^{(l+1,p)}) \cdot \delta^{(l+1,p)}
\end{aligned}
$$

$ROT180$ 表示旋转 $180$ 度

### 池化层

池化层误差项递推为

$$
\delta^{(l,p)}=upsample(\delta^{(l+1,p)}) \cdot f^{\prime}
$$

# 循环神经网络

> [!NOTE]
> TODO

# 网络优化与正则化
