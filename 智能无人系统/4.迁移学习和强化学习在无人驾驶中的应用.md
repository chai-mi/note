# 速记模块（顺口溜总结）

## 总复习 PPT 模块

- 迁移学习
  - 迁移学习的必要性
    - 矛盾 - 传统 - 迁移
    - 数少数弱铺个特应需求
  - 迁移学习与传统机器学习的对比
    - 分布标注模型
  - 迁移学习方法分类
    - 实例特征模型关系（管事特么多）
  - 迁移学习应用场景
- 强化学习
  - 端到端无人驾驶思路与模型
  - 马尔可夫性和马尔可夫过程
  - 非确定性策略和确定性策略
  - 及时奖励和累计期望奖励
  - 状态 - 值函数和状态 - 行为值函数之间的关系
  - 请解释贝尔曼方程，并说明其在强化学习中的作用
  - DQN 和 Q-Learing 算法的区别是什么
  - DQN 中的经验回放起到什么作用
  - DQN 中的目标网络和评估网络有何区别及联系，分别承担什么作用
  - 策略梯度
    - 最大似然和蒙特卡洛法计算梯度得到最终梯度公式
    - 如果换成 t 时刻的奖励会发生什么后果
  - 深度确定性策略梯度算法（DDPG）使用演员 - 评论家（Actor-Critic）算法作为其基本框架，采用深度神经网络作为策略网络和动作值函数的近似。对于策略函数和价值函数均使用双重神经网络模型架构，请解释 Critic 目标网络和训练网络、Actor 目标网络和训练网络分别的作用

# 迁移学习学习和端到端无人驾驶

## 迁移学习

### 目的

用于**解决标记数据难获取**这一基础问题的重要手段

### 什么是迁移学习

通过减小源域 (辅助领域) 到目标域的分布差异，进行知识迁移，从而实现数据标定

- 源域——>目标域 (减少差异，知识迁移)

> [!NOTE]
> 从别人数据学习自己的特征，要减少差异。
>
> - 背景，原数据量中目标域的数据很少，但源域 (辅助领域) 的数据量多
> - 源域 (辅助领域) ，即为给出的数据中不是目标类别的数据，但是为其相似的类别的数据
> - 目标域，即为真正属于该类别的数据

### 核心思想 (关键)

- 找到不同任务之间的相关性
- 举一反三、找猫画虎、不要东施效颦 (负迁移)

### 意义

- 解决大数据与少标签之间的矛盾
- 解决大数据与弱计算之间的矛盾
- 解决普适化模型与个性化需求之间的矛盾 (云 + 端的模型)
- 满足特定应用的需求

| 矛盾 | 传统机器学习 | 迁移学习 |
|---|---|---|
| 大数据与少标注 | 增加人工标注，但是昂贵且耗时 | 数据的迁移标注 |
| 大数据与弱计算 | 只能依赖强大计算能力，但是受众少 | 模型迁移 |
| 普适化模型与个性化需求 | 通用模型无法满足个性化需求 | 模型自适应调整 |
| 特定应用 | 冷启动问题无法解决 | 数据迁移 |

### 迁移学习与传统机器学习的对比

| 比较项目 | 传统机器学习 | 迁移学习 |
|---|---|---|
| 数据分布 | 训练和测试数据服从相同的分布 | 训练和测试数据服从不同的分布 |
| 数据标注 | 需要足够的标注来训练模型 | 不需要足够的标注 |
| 模型 | 每个任务分别建模 | 模型可以在不同任务之间迁移 |

### 迁移学习方法研究领域

#### 迁移学习研究领域与方法分类

![alt text](./image/迁移学习研究领域与方法分类.png)

#### 迁移学习方法分类

- 基于实例的迁移 (instance based TL)
  - 通过权重重新利用源域和目标域的样例进行迁移
- 基于特征的迁移 (feature based TL)
  - 将源域和目标域的特征变换到相同空间
- 基于模型的迁移 (parameter based TL)
  - 利用源域和目标域的参数共享模型
- 基于关系的迁移 (relation based TL)
  - 利用源域中的逻辑网络关系进行迁移

#### 迁移学习应用场景

在开发深度学习模型时，并不一定需要从头开始训练模型。迁移学习可以方便将这些经典模型移植到新的应用场景中

如何移植取决于两个因素
- 新应用数据量大小
- 新应用和原始模型的相似度

| 数据集数量 | 应用相似度 | 模型训练方法 |
|---|---|---|
| 较大 | 较高 | 微调模型 |
| 较大 | 较低 | 微调或者重新训练 |
| 较小 | 较高 | 对全连接分类层进行修改训练 |
| 较小 | 较低 | 重新设计、重新训练模型 |

## 端到端无人驾驶

### 基本思路

- 采集数据：人为操作车辆 (模拟器) 行驶来采集控制数据
- 网络训练：
  - 训练深层神经网络模型
  - 采集到的路况图像数据作为模型的输入参数
  - 汽车的控制参数作为模型的输出数据
- 测试
  - 基于这些路况图像和控制参数训练完神经网络模型，具有对路况的预测能力
  - 给出车辆的控制参数，输入汽车线控单元，从而达到控制车辆自动行驶的目的

#### 模型框架

![端对端模型框架](./image/端对端模型框架.png)

## 端到端无人驾驶模拟

略

# 强化学习和端到端无人驾驶

## 强化学习概述

- 来源于心理学中的**行为主义**理论
- 强化学习没有监督者，只有一个**奖励信号**，而且反馈不是立即生成的，而是**延迟**的，因此时间在强化学习中具有非常重要的意义
- 强化学习并不需要带标签的数据，有**可交互的环境**即可

## 强化学习原理及过程

- 个体，Agent，学习器角色，也称为智能体
- 环境，Environment，Agent 之外一切组成的、与之交互的事物
- 动作，Action，Agent 的行为
- 状态，State，Agent 从环境获取的信息
- 奖励，Reward，环境对于动作的反馈

![强化学习原理及过程](./image/强化学习原理及过程.png)

智能体首先从环境中获取一个状态 $O_t$，然后根据 $O_t$ 调整自身的策略做出行为 $A_t$ 并反馈给环境，环境根据智能体的动作给予智能体一个奖励 R，智能体和环境之间通过不断地交互学习，得到一个 { $O_t,A_t,R$ } 的交互历史序列

### 马尔可夫决策过程

强化学习的本质是一个序列决策过程，通过不断地学习每个状态，最终得到一个最佳的序列 (策略)，这与马尔可夫决策过程解决的问题相似

#### 马尔可夫性

指系统的下一个状态 $S_{t+1}$ 仅与当前状态 $S_t$ 有关，表示为

$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, S_2, ..., S_t)
$$

#### 马尔可夫过程

它是一个二元组 (S, P)，其中 S 为有限状态机，P 为状态转移概率矩阵，该矩阵为：

$$
P =
\begin{pmatrix}
P_{11} & \dots & P_{1n} \\
\vdots & \ddots & \vdots \\
P_{n1} & \dots & P_{nn}
\end{pmatrix}
$$

在强化学习中，问题被描述为一个马尔可夫决策过程 (Markov Decision Process, MDP)
- 由一个元组<S, A, P, R>表示
  - S 为有限状态空间集合， $s_t \in S$ 表示 t 时刻状态
  - A 为有限动作空间集合， $a_t \in A$ 表示 t 时刻动作
  - P 为状态转移概率，P(s'|s, a) 表示在状态 s 下执行动作 a 后，转移至下一状态 s'的概率
  - R 为奖赏函数，执行完动作转移至下一状态时，奖励记作 R = r(s'|s, a)
   ![MDP 图解](./image/MDP图解.png)

<details>

<summary>例题</summary>

给定状态转移概率矩阵 P，从 Start 到 End 是一个序列决策问题，从 Start 出发到 End 结束存在多条马尔可夫链 (路径)，其中每条链上就是马尔可夫过程的描述

![马尔可夫过程例题](./image/马尔可夫过程例题.png)

但马尔可夫过程中并不存在行为和奖励。因此，从 Start 到 End 是一个序列决策问题，需要不断地选择路线以取得最大收益 (马尔可夫决策过程)

$$
Start-A-B-E-End
$$

$$
{(Start, East, P, R1),(A, East, P, R2),(B, South, P, R3),(E, South, P, R4),(End, South, P, R5)}
$$

</details>

#### 非确定性策略与确定性策略

带来最大收益需要一个策略，定义为**从状态到行为的一个映射**
- 即为每个状态下指定一个动作概率
  - 非确定性策略
    - 对于相同的状态，其输出的状态**并不唯一**，而是满足一定的概率分布，从而导致即使处于相同的状态，也可能输出不同的动作，表示为

        $$
        \pi(a|s) = P[A_t = a | S_t = s]
        $$

  - 确定性策略
    - 在相同的状态下，其输出的动作是**确定的**，实际上，只需要建立一个神经网络，输入状态，输出一个确定的动作就行。表示为

        $$
        a = \mu(s)
        $$

#### 及时奖励与累计期望奖励

给定一个策略，就可以计算最大化累计期望奖励
- 及时奖励
  - 即时反馈给智能体的奖励，如当玩家直接根据当前的状态做出一个飞行控制决策时，会立即得到一个奖励
- 累积期望奖励
  - 指一个过程的总奖励期望，例如直升机从起飞落地的过程。一般情况下，累积期望奖励被定义为：

    $$
    G_t = R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots + \gamma^{k-1} R_k = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
    $$

    其中，$\gamma$ 为折扣因子

    假如在策略 $\pi$ 下，从前文的 Start 出发，则有不同的路径。每个路径的累积回报 $G_t$ 不同，因此随机变量 $G_t$ 是随机变量，但它们的期望是一个确定值，因此需要对值函数进行估计

#### 值函数

- 每一个动作都要以最终的目标——**最大化长期回报**为目标
- 量化某一时刻的回报（奖励）值，可以利用它，将其扩展为值函数
- 累计回报并不简单，主要反映在计算的时间跨度
  - 有限时间，计算复杂但可计算
  - 无限时间，计算累积回报没有意义
- 为了解决该问题，需要降低未来回报对当前时刻状态的影响，即对未来回报乘以一个 0-1 的系数

将状态$s$的期望值视为状态 - 值函数，数学表达为：

$$
V_{\pi}(s) = {E}_{\pi}[R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots + \gamma^{k-1} R_k | S_t = s]
$$

我们还需要知道在某一个状态下采取某行为会带来的期望回报

用状态 - 行为值函数衡量当前行为的好坏，其数学表达式为

$$
q_{\pi}(s, a) = {E}_{\pi} \left[ \sum_{k=0}^{\infty} \gamma^k R_{t+k+1} | S_t = s, A_t = a \right]
$$

在一般情况下，状态 - 值函数和状态 - 行为值函数之间的关系表示为：

$$
V_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)
$$

<details>

<summary>状态 - 值函数和状态 - 行为值函数之间的关系图及其解释</summary>

![状态 - 值函数和状态 - 行为值函数之间的关系图](./image/状态值函数和状态行为值函数之间的关系图.png)

- 解释

    为了能够计算在某个状态 $S_t$ 下的值函数或在状态 $S_t$ 下**采取**行为的状态 - 动作值函数的累积期望奖励，根据图推导得到 $V_{\pi}$ 和 $q_{\pi}$ 的关系如下：

    $$
    q_{\pi}(s, a) = R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} V_{\pi}(s')
    $$

    $$
    V_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)
    $$

    将上述公式互相代入得到：

    $$
    V_{\pi}(s) = \sum_{a \in A} \pi(a|s) \left( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} V_{\pi}(s') \right)
    $$

    $$
    q_{\pi}(s, a) = R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} \sum_{a' \in A} \pi(a'|s') q_{\pi}(s', a')
    $$

    公式互相代入得到：**Bellman 期望方程**

    $$
    V_{\pi}(s_t) = {E}_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1})]
    $$

    $$
    q_{\pi}(s_t, a_t) = {E}[R_{t+1} + \gamma V_{\pi}(S_{t+1})]
    $$

</details>

#### 请解释贝尔曼方程，并说明其在强化学习中的作用

## Q-learning

## 近似价值函数

## 深度 Q 值网络算法

## 策略梯度

## 深度确定性策略梯度算法

## 深度强化学习在自动驾驶中的应用
