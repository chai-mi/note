# 速记模块（顺口溜总结）

## 总复习 PPT 模块

- [速记模块（顺口溜总结）](#速记模块顺口溜总结)
  - [总复习 PPT 模块](#总复习-ppt-模块)
- [迁移学习学习和端到端无人驾驶](#迁移学习学习和端到端无人驾驶)
  - [迁移学习](#迁移学习)
    - [目的](#目的)
    - [什么是迁移学习](#什么是迁移学习)
    - [核心思想 (关键)](#核心思想-关键)
    - [意义](#意义)
    - [迁移学习与传统机器学习的对比](#迁移学习与传统机器学习的对比)
    - [迁移学习方法研究领域](#迁移学习方法研究领域)
      - [迁移学习研究领域与方法分类](#迁移学习研究领域与方法分类)
      - [迁移学习方法分类](#迁移学习方法分类)
      - [迁移学习应用场景](#迁移学习应用场景)
  - [端到端无人驾驶](#端到端无人驾驶)
    - [基本思路](#基本思路)
      - [模型框架](#模型框架)
  - [端到端无人驾驶模拟](#端到端无人驾驶模拟)
- [强化学习和端到端无人驾驶](#强化学习和端到端无人驾驶)
  - [强化学习概述](#强化学习概述)
  - [强化学习原理及过程](#强化学习原理及过程)
    - [马尔可夫决策过程](#马尔可夫决策过程)
      - [马尔可夫性](#马尔可夫性)
      - [马尔可夫过程](#马尔可夫过程)
      - [非确定性策略与确定性策略](#非确定性策略与确定性策略)
      - [非确定性策略](#非确定性策略)
      - [确定性策略](#确定性策略)
      - [及时奖励与累计期望奖励](#及时奖励与累计期望奖励)
      - [及时奖励](#及时奖励)
      - [累积期望奖励](#累积期望奖励)
      - [值函数](#值函数)
      - [状态 - 值函数和状态 - 行为值函数之间的关系图及其解释](#状态---值函数和状态---行为值函数之间的关系图及其解释)
      - [要点回顾](#要点回顾)
      - [请解释贝尔曼方程，并说明其在强化学习中的作用](#请解释贝尔曼方程并说明其在强化学习中的作用)
  - [Q-learning](#q-learning)
    - [Q-Learing 伪代码](#q-learing-伪代码)
  - [近似价值函数](#近似价值函数)
  - [深度 Q 值网络算法](#深度-q-值网络算法)
    - [DQN 和 Q-Learing 算法的区别](#dqn-和-q-learing-算法的区别)
    - [推导过程](#推导过程)
    - [奖励函数](#奖励函数)
    - [目标函数](#目标函数)
    - [经验回放和目标网络](#经验回放和目标网络)
    - [DQN 的目标网络和评估网络有何区别及联系](#dqn-的目标网络和评估网络有何区别及联系)
  - [策略梯度](#策略梯度)
    - [如果换成 t 时刻的奖励会发生什么后果](#如果换成-t-时刻的奖励会发生什么后果)
  - [深度确定性策略梯度算法](#深度确定性策略梯度算法)
    - [DDPG 算法结构](#ddpg-算法结构)
      - [解释 Critic 目标网络和训练网络、Actor 目标网络和训练网络分别的作用](#解释-critic-目标网络和训练网络actor-目标网络和训练网络分别的作用)
      - [伪代码](#伪代码)
  - [深度强化学习在自动驾驶中的应用](#深度强化学习在自动驾驶中的应用)

# 迁移学习学习和端到端无人驾驶

## 迁移学习

### 目的

用于**解决标记数据难获取**这一基础问题的重要手段

### 什么是迁移学习

通过减小源域 (辅助领域) 到目标域的分布差异，进行知识迁移，从而实现数据标定

- 源域——>目标域 (减少差异，知识迁移)

> [!NOTE]
> 从别人数据学习自己的特征，要减少差异。
>
> - 背景，原数据量中目标域的数据很少，但源域 (辅助领域) 的数据量多
> - 源域 (辅助领域) ，即为给出的数据中不是目标类别的数据，但是为其相似的类别的数据
> - 目标域，即为真正属于该类别的数据

### 核心思想 (关键)

- 找到不同任务之间的相关性
- 举一反三、找猫画虎、不要东施效颦 (负迁移)

### 意义

- 解决大数据与少标签之间的矛盾
- 解决大数据与弱计算之间的矛盾
- 解决普适化模型与个性化需求之间的矛盾 (云 + 端的模型)
- 满足特定应用的需求

| 矛盾                   | 传统机器学习                     | 迁移学习       |
| ---------------------- | -------------------------------- | -------------- |
| 大数据与少标注         | 增加人工标注，但是昂贵且耗时     | 数据的迁移标注 |
| 大数据与弱计算         | 只能依赖强大计算能力，但是受众少 | 模型迁移       |
| 普适化模型与个性化需求 | 通用模型无法满足个性化需求       | 模型自适应调整 |
| 特定应用               | 冷启动问题无法解决               | 数据迁移       |

### 迁移学习与传统机器学习的对比

| 比较项目 | 传统机器学习                 | 迁移学习                     |
| -------- | ---------------------------- | ---------------------------- |
| 数据分布 | 训练和测试数据服从相同的分布 | 训练和测试数据服从不同的分布 |
| 数据标注 | 需要足够的标注来训练模型     | 不需要足够的标注             |
| 模型     | 每个任务分别建模             | 模型可以在不同任务之间迁移   |

### 迁移学习方法研究领域

#### 迁移学习研究领域与方法分类

![alt text](./image/迁移学习研究领域与方法分类.png)

#### 迁移学习方法分类

- 基于实例的迁移 (instance based TL)
  - 通过权重重新利用源域和目标域的样例进行迁移
- 基于特征的迁移 (feature based TL)
  - 将源域和目标域的特征变换到相同空间
- 基于模型的迁移 (parameter based TL)
  - 利用源域和目标域的参数共享模型
- 基于关系的迁移 (relation based TL)
  - 利用源域中的逻辑网络关系进行迁移

#### 迁移学习应用场景

在开发深度学习模型时，并不一定需要从头开始训练模型。迁移学习可以方便将这些经典模型移植到新的应用场景中

如何移植取决于两个因素
- 新应用数据量大小
- 新应用和原始模型的相似度

| 数据集数量 | 应用相似度 | 模型训练方法               |
| ---------- | ---------- | -------------------------- |
| 较大       | 较高       | 微调模型                   |
| 较大       | 较低       | 微调或者重新训练           |
| 较小       | 较高       | 对全连接分类层进行修改训练 |
| 较小       | 较低       | 重新设计、重新训练模型     |

## 端到端无人驾驶

### 基本思路

- 采集数据
  - 人为操作车辆 (模拟器) 行驶来采集控制数据
- 网络训练
  - 训练深层神经网络模型
  - 采集到的路况图像数据作为模型的输入参数
  - 汽车的控制参数作为模型的输出数据
- 测试
  - 基于这些路况图像和控制参数训练完神经网络模型，具有对路况的预测能力
  - 给出车辆的控制参数，输入汽车线控单元，从而达到控制车辆自动行驶的目的

#### 模型框架

![端对端模型框架](./image/端对端模型框架.png)

## 端到端无人驾驶模拟

略

# 强化学习和端到端无人驾驶

## 强化学习概述

- 来源于心理学中的**行为主义**理论
- 强化学习没有监督者，只有一个**奖励信号**，而且反馈不是立即生成的，而是**延迟**的，因此时间在强化学习中具有非常重要的意义
- 强化学习并不需要带标签的数据，有**可交互的环境**即可

## 强化学习原理及过程

- 个体，Agent，学习器角色，也称为智能体
- 环境，Environment，Agent 之外一切组成的、与之交互的事物
- 动作，Action，Agent 的行为
- 状态，State，Agent 从环境获取的信息
- 奖励，Reward，环境对于动作的反馈

![强化学习原理及过程](./image/强化学习原理及过程.png)

智能体首先从环境中获取一个状态 $O_t$，然后根据 $O_t$ 调整自身的策略做出行为 $A_t$ 并反馈给环境，环境根据智能体的动作给予智能体一个奖励 R，智能体和环境之间通过不断地交互学习，得到一个 { $O_t,A_t,R$ } 的交互历史序列

### 马尔可夫决策过程

强化学习的本质是一个序列决策过程，通过不断地学习每个状态，最终得到一个最佳的序列 (策略)，这与马尔可夫决策过程解决的问题相似

#### 马尔可夫性

指系统的下一个状态 $S_{t+1}$ 仅与当前状态 $S_t$ 有关，表示为

$$
P(S_{t+1}|S_t) = P(S_{t+1}|S_1, S_2, ..., S_t)
$$

#### 马尔可夫过程

它是一个二元组 (S, P)，其中 S 为有限状态机，P 为状态转移概率矩阵，该矩阵为：

$$
P =
\begin{pmatrix}
P_{11} & \dots & P_{1n} \\
\vdots & \ddots & \vdots \\
P_{n1} & \dots & P_{nn}
\end{pmatrix}
$$

在强化学习中，问题被描述为一个马尔可夫决策过程 (Markov Decision Process, MDP)
- 由一个元组<S, A, P, R>表示
  - S 为有限状态空间集合， $s_t \in S$ 表示 t 时刻状态
  - A 为有限动作空间集合， $a_t \in A$ 表示 t 时刻动作
  - P 为状态转移概率，P(s'|s, a) 表示在状态 s 下执行动作 a 后，转移至下一状态 s'的概率
  - R 为奖赏函数，执行完动作转移至下一状态时，奖励记作 R = r(s'|s, a)

    ![MDP 图解](./image/MDP图解.png)

<details>

<summary>例题</summary>

给定状态转移概率矩阵 P，从 Start 到 End 是一个序列决策问题，从 Start 出发到 End 结束存在多条马尔可夫链 (路径)，其中每条链上就是马尔可夫过程的描述

![马尔可夫过程例题](./image/马尔可夫过程例题.png)

但马尔可夫过程中并不存在行为和奖励。因此，从 Start 到 End 是一个序列决策问题，需要不断地选择路线以取得最大收益 (马尔可夫决策过程)

Start-A-B-E-End

{(Start, East, P, R1),(A, East, P, R2),(B, South, P, R3),(E, South, P, R4),(End, South, P, R5)}

</details>

#### 非确定性策略与确定性策略

带来最大收益需要一个策略，定义为**从状态到行为的一个映射**，即为每个状态下指定一个动作概率

#### 非确定性策略

对于相同的状态，其输出的状态**并不唯一**，而是满足一定的概率分布，从而导致即使处于相同的状态，也可能输出不同的动作，表示为

$$
\pi(a|s) = P[A_t = a | S_t = s]
$$

#### 确定性策略

在相同的状态下，其输出的动作是**确定的**，实际上，只需要建立一个神经网络，输入状态，输出一个确定的动作就行。表示为

$$
a = \mu(s)
$$

#### 及时奖励与累计期望奖励

给定一个策略，就可以计算最大化累计期望奖励

#### 及时奖励

即时反馈给智能体的奖励，如当玩家直接根据当前的状态做出一个飞行控制决策时，会立即得到一个奖励

#### 累积期望奖励

指一个过程的总奖励期望，例如直升机从起飞落地的过程。一般情况下，累积期望奖励被定义为

$$
G_t = R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots + \gamma^{k-1} R_k = \sum_{k=0}^{\infty} \gamma^k R_{t+k+1}
$$

其中， $\gamma$ 为折扣因子

假如在策略 $\pi$ 下，从前文的 Start 出发，则有不同的路径。每个路径的累积回报 $G_t$ 不同，因此随机变量 $G_t$ 是随机变量，但它们的期望是一个确定值，因此需要对值函数进行估计

#### 值函数

- 每一个动作都要以最终的目标——**最大化长期回报**为目标
- 量化某一时刻的回报（奖励）值，可以利用它，将其扩展为值函数
- 累计回报并不简单，主要反映在计算的时间跨度
  - 有限时间，计算复杂但可计算
  - 无限时间，计算累积回报没有意义
- 为了解决该问题，需要降低未来回报对当前时刻状态的影响，即对未来回报乘以一个 0-1 的系数

将状态 $s$ 的期望值视为状态 - 值函数，数学表达为

$$
V_{\pi}(s) = E_{\pi}[R_1 + \gamma R_2 + \gamma^2 R_3 + \ldots + \gamma^{k-1} R_k | S_t = s]
$$

我们还需要知道在某一个状态下采取某行为会带来的期望回报

用状态 - 行为值函数衡量当前行为的好坏，其数学表达式为

$$
q_{\pi}(s, a) = E_{\pi}[\sum_{k=0}^{\infty} \gamma^{k} R_{t+k+1} | S_t = s, A_t = a]
$$

在一般情况下，状态 - 值函数和状态 - 行为值函数之间的关系表示为

$$
V_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)
$$

#### 状态 - 值函数和状态 - 行为值函数之间的关系图及其解释

![状态 - 值函数和状态 - 行为值函数之间的关系图](./image/状态值函数和状态行为值函数之间的关系图.png)

> 详细公式推导查看 [此处](https://blog.csdn.net/Mocode/article/details/130383093)

解释

为了能够计算在某个状态 $S_t$ 下的值函数或在状态 $S_t$ 下**采取**行为的状态 - 动作值函数的累积期望奖励，根据图推导得到 $V_{\pi}$ 和 $q_{\pi}$ 的关系如下

$$
q_{\pi}(s, a) = R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} V_{\pi}(s')
$$

$$
V_{\pi}(s) = \sum_{a \in A} \pi(a|s) q_{\pi}(s, a)
$$

将上述公式互相代入得到

$$
V_{\pi}(s) = \sum_{a \in A} \pi(a|s) \left( R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} V_{\pi}(s') \right)
$$

$$
q_{\pi}(s, a) = R^a_s + \gamma \sum_{s' \in S} P^a_{ss'} \sum_{a' \in A} \pi(a'|s') q_{\pi}(s', a')
$$

公式互相代入得到**Bellman 期望方程**

$$
V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s]
$$

$$
q_{\pi}(s_t, a_t) = E[R_{t+1} + \gamma q_{\pi}(s_{t+1}, a_{t+1})]
$$

计算值函数的目的是为了从数据中学到最优策略，每个策略对应一个值函数，最优策略对应最优值函数

$$
V_\pi(s) = \max_\pi R_s^a + \gamma \sum_{s' \in S}P_{ss'}^aV^*(s')
$$

$$
q_\pi(s, a) = R_s^a + \gamma \sum_{s' \in S}P_{ss'}^a\max_a q^*(s', a)
$$

如果已知最优状态 - 行为值函数，最优策略可以直接通过最大化 $q^*(s,a)$ 得到

$$
\pi^\star(a|s) = I [a = \arg \max_{a'} q^\star(s, a')]
$$

#### 要点回顾

状态、动作、奖励、策略、延迟奖励（引入状态价值函数）、奖励衰减因子、状态转化模型、探索率

#### 请解释贝尔曼方程，并说明其在强化学习中的作用

贝尔曼方程用于描述马尔可夫决策过程 (MDP) 中状态值或动作状态值的关系
- 状态值函数表示在状态 s 下，遵循某一策略 $\pi$ 的期望回报

$$
V_{\pi}(s) = E_{\pi}[R_{t+1} + \gamma V_{\pi}(S_{t+1}) | S_t = s]
$$

- 动作值函数，表示在状态 s 下采取动作 a 的期望回报

$$
q_{\pi}(s_t, a_t) = E[R_{t+1} + \gamma q_{\pi}(s_{t+1}, a_{t+1})]
$$

- 在强化学习中的作用
  - 最优策略的求解
    - 贝尔曼方程提供了一种递归结构，价值迭代与策略迭代使得可以通过迭代的方法寻求最优策略
  - 模型自由学习
    - 在强化学习的在线学习场景中，可以使用贝尔曼方程的无模型版本，如 Q-learning 中，来在没有明确模型的情况下学习最优策略
  - 理论基础
    - 贝尔曼方程为强化学习提供了理论框架，帮助理解价值函数和策略之间的关系，为算法设计提供指导

## Q-learning

Q-Learning 算法就是一种 value-based 的强化学习算法
- 初始化 Q-table (刚开始，完全随机产生)
- 算法利用贝尔曼方程来迭代更新 Q(s,a)，每一轮结束后就生成了一个新的 Q-table
- agent 不断与环境进行交互，不断更新这个表格，使其最终能收敛
- 最终 agent 就能通过表格判断在某个转态 s 下采取什么动作，才能获得最大的 Q 值

更新过程

$$
Q(s_t, a_t) \leftarrow Q(s_t, a_t) + \alpha [r_{t+1} + \gamma \max_a Q(s_{t+1}, a) - Q(s_t, a_t)]
$$

- $Q(s_t, a_t)$ 是在状态 $s_t$ 下采取动作 $a_t$ 的长期回报，是一个估计 Q 值
- $r_{t+1}$ 是在状态 $s_t$ 下采取动作 $a_t$ 得到的回报 reward
- $\max_a Q(s_{t+1}, a)$ 指的是在状态 $s_{t+1}$ 下所获得的最大 Q 值，直接查看 Q-table 取它的最大化的值。 $\gamma$ 是折扣因子，含义是看重近期收益，弱化远期收益，同时也保证 Q 函数收敛
- $r_{t+1} + \gamma \max_a Q(s_{t+1}, a)$ 即为目标值，也就是**时序差分目标**，是 $Q(s_t, a_t)$ 想要逼近的目标。 $\alpha$ 是学习率，衡量更新的幅度
- $\max_a Q(s_{t+1}, a)$ 所对应的动作不一定是下一步会执行的实际动作

这里引出 $\epsilon-greedy$，即 $\epsilon$-贪心算法
- exploration 探索环境，收敛速度慢
- exploitation 不尝试新动作，局部最优

### Q-Learing 伪代码

![Q-Learing 伪代码](./image/Q-Learing伪代码.png)

## 近似价值函数

近似价值函数就是无限逼近真实价值函数状态 - 值函数 V(s)、状态 - 行为值函数 Q(s , a)

$$
\hat{v}(s, w) \approx v^{\pi}(s)
$$

$$
\hat{q}(s, a, w) \approx q^{\pi}(s, a)
$$

$$
\hat{\pi}(a, s, w) \approx \pi(a|s)
$$

函数近似的目的就是找到一个合适的参量 $w$ 来近似值函数

几种常见的近似函数形式
- 线性特征组合 (Linear Combination of Features)
- 神经网络 (Neural Network)
- 决策树 (Decision Tree)
- 最近邻 (Nearest Neighbor)

其中，前两种近似函数属于可微函数，利用此性质可以很好地进行优化

近似价值函数的类别

![近似价值函数的类别](./image/近似价值函数的类别.png)

- 根据状态本身，输出这个状态的近似价值
- q 函数，将 state 和 action 作为输入，然后输出是给定状态和 action 的价值是多少
- q 函数，输入是状态，输出是对于所有 action 可能的 q 值，然后再输出后取 argmax，把最可能的 action 选出来

## 深度 Q 值网络算法

DQN，即深度 Q 网络 (Deep Q-network)，是指基于深度学习的 Q-Learing 算法
- 强化学习任务所面临的状态空间是**连续的**，不能再使用表格的方式存储价值函数
- 用一个函数 Q(s,a,w) 来近似动作价值 Q(s,a)
- 用神经网络来生成这个函数 Q(s,a,w)，称为 Q 网络，w 是神经网络训练的参数

### DQN 和 Q-Learing 算法的区别

- Q-learning：状态空间是**离散的**，算法维护一个 Q-table，Q-table 记录了不同状态下 s( $s \in S$ )，采取不同动作 a ($a \in A$) 的所获得的 Q 值
- DQN：是基于深度学习的 Q-learning 算法，状态空间是**连续的**，不能用表格描述，**使用神经网络生成函数 Q(s,a,w) 近似动作价值函数**

### 推导过程

目标：更新当前状态 $S_t$ 下采取动作 $A$ 的 Q 值： $Q(S, A)$

1. 执行动作 $A$，往前一步，到达 $S_{t+1}$
2. 把 $S_{t+1}$ 输入 Q 网络，计算 $S_{t+1}$ 下所有动作的 Q 值
3. 获得最大的 Q 值加上奖励 $R$ 作为更新目标
4. 计算损失 $r_t + \gamma \max_a Q(S_{t+1}, a) - Q(S_t, A)$
5. 用 loss 更新 Q 网络

<details>

<summary>DQN 推导图解</summary>

![DQN 推导图解](./image/推导图解.png)

</details>

### 奖励函数

- “奖励 & 收益”其实是智能体目标的一种**形式化、数值化**的表征
- 奖励函数的本质是向智能体传达目标

### 目标函数

采用梯度下降最小化目标函数来不断地更新网络权重 $\theta$

定义

$$
L_i(\theta_i) = E_{(s, a, r, s')\sim U(D)}[(r + \gamma \max_{a'} Q(s', a'; \theta_i^-) - Q(s, a; \theta_i))^2]
$$

其中 $\theta_i^{-}$ 是第 i 次迭代的目标网络参数；

$\theta_i$ 是 Q-network 网络参数，通过梯度下降法对损失函数求梯度得到

$$
\frac{\partial L_i(\theta_i)}{\partial \theta_i} = L_i(\theta_i) = E_{(s, a, r, s')\sim U(D)}[(r + \gamma \max_{a'} \hat{Q}(s', a'; \theta_i^-) - Q(s, a; \theta_i))\nabla_{\theta_i}Q(s, a; \theta_i)]
$$

### 经验回放和目标网络

- 经验回放
  - 基本思想
    - 将智能体的交互经验存储在一个经验池中，并在智能体进行更新时从经验池中随机采样一小批经验来进行训练。
  - 作用
    - 智能体可以**更好地利用过去的经验，提高学习效率，降低了采样成本**。
    - 通过随机采样，经验回放可以**减少训练数据之间的相关性，提高算法收敛速度**。

- 目标网络
  - 使用的第二个网络，用来计算 $TDtarget: y_t = r_t + \gamma \max_\pi Q(S_{t+1}, a_t)$
  - 目标网络是提供一个稳定的目标 Q 值，以减少训练中的估计偏差

自举问题 (Bootstrapping)，在强化学习中，自举是指用后继的估算值，来更新现在状态的估算值

$$
TDtarget: y_{t} = r_{t} + \gamma\max_\pi Q(S_{t+1}, a_{t})
$$

- $r_t$ 是根据实际观测得到的值
- $\max_\pi Q(S_{t+1}, a_t)$ 是根据 Q 网络在 $S_{t+1}$ 的做出的估计值
- 因此 $y_t$ 有部分是来自 Q 网络的估算，而我们用 $y_t$ 来更新 Q 网络本身，这源于自举

### DQN 的目标网络和评估网络有何区别及联系

- 目标网络和评估网络 $Q(s, a, \theta)$ 结构一样，只是参数不同 $\theta^- \neq \theta$，参数更新频率也不同
  - 评估网络 $Q(s, a, \theta)$ 的权重 $\theta$ 会随着训练而不断更新，逼近最优 Q 值函数，
  - 目标网络的参数 $\theta^-$ 在一定时间内保持固定，降低当前 Q 值和目标 Q 值的相关性
- 两个网络的作用不同
  - 评估网络用于估计 Q 值函数，收集经验，并决定智能体的动作选择。
  - 目标网络则用于提供一个稳定的目标 Q 值，以减少训练中的估计偏差，从而使学习过程更加稳定。

<details>

<summary>目标网络和评估网络的流程</summary>

![目标网络和评估网络的流程](./image/目标网络和评估网络的流程.png)

在更新过程中，评估网络 $Q(s, a, \theta)$ 的权重 $\theta$ 会随着训练而不断更新，以逼近最优 Q 值函数。而目标网络的参数 $\theta^-$  则在一定时间内保持固定，然后周期性地用评估网络的参数更新，例如 $\theta^- \leftarrow \theta$。这在一定程度上降低了当前 Q 值和目标 Q 值的相关性，提高了算法稳定性

</details>

## 策略梯度

策略梯度不计算奖励，而是输出选择所有动作的概率分布，然后基于概率选择动作

为了防止**相同的可感知的状态**下总是**做出固定的判断**而导致进入死循环

通过最大似然和蒙卡洛法计算梯度得到最终公式

$$
\nabla_\theta J(\theta) = \frac{1}{N} \sum_{i=1}^T \left[\sum_{t=0}^T \nabla_\theta \log \pi_\theta(a_i, t|s_i, t) (\sum_{t=0}^T r(s_i, t, a_i, t))\right]
$$

- $\pi_\theta(a_{i,t}|s_{i,t})$ 表示在策略 $\pi_\theta$  下，给定状态  $s_{i,t}$ 时，智能体采取动作 $a_{i,t}$ 的概率
- $\sum_{t=0}^T r(s_i, t, a_i, t)$ 箭 n 回环游戏**总的奖励**
- 到此就可以计算策略梯度 $\nabla_\theta J(\theta)$和更新策略梯度为

$$
\theta = \theta + \alpha \nabla_\theta J(\theta)
$$

在以上基础上，以及保持独立同分布的前提下就可以使用神经网络通过策略梯度

### 如果换成 t 时刻的奖励会发生什么后果

- 会导致只注重当前做的动作的回报，而不会注重整个游戏的过程中的奖励
- 容易陷入局部最优，无法更新到最优解
- 比如在射击游戏里，只有开火能得到奖励，那么就会导致机器只会开火

## 深度确定性策略梯度算法

DDPG 算法是对 DQN 的一种改进，是一种无模型的深度强化学习算法

- DDPG 算法使用演员 - 评论家（Actor-Critic）算法作为其基本框架
- 采用深度神经网络作为策略网络和动作值函数的近似
- 使用随机梯度法训练策略网络和价值网络模型中的参数

### DDPG 算法结构

- 使用双重神经网络架构
- 策略函数和价值函数均使用双重神经网络模型架构
  - 学习过程更加稳定，收敛的速度加快时引入经验回放机制
  - 去除样本的相关性和依赖性

<details>

<summary>深度确定性算法结构图解</summary>

![深度确定性算法结构](./image/深度确定性算法结构.png)

</details>

#### 解释 Critic 目标网络和训练网络、Actor 目标网络和训练网络分别的作用

![深度确定性算法网络](./image/深度确定性算法网络.png)

DDPG 共包含 4 个神经网络，用于对 Q 值函数和策略的近似表示
- Critic (评论家)，用于对当前策略评价
  - 目标网络用于近似估计下一时刻的状态 - 动作的 Q 值函数，
  - 训练网络输出当前时刻状态 - 动作的 Q 值函数
- Actor (演员)，结合 Critic 训练网络的 Q 值函数可以得到 Actor 在参数更新时的策略梯度
  - 目标网络用于提供下一个状态的策略
  - 训练网络则是提供当前状态的策略

#### 伪代码

![深度确定性算法伪代码](./image/深度确定性算法伪代码.png)

## 深度强化学习在自动驾驶中的应用

略
